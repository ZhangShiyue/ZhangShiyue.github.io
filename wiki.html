<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Wikipedia</title>
     <link rel="stylesheet" href="css/templatemo-style.css">
</head>
<body>
<h2 class="tm-text-title">Wikipedia</h2>

<img src="img/wiki.png"> <br>
<div class="tm-bg-white-translucent text-xs-left tm-textbox tm-2-col-textbox">
    <p class="tm-text">
        Figure above shows what I have done and what I'm doing in this project. First, I designed a small wiki api
        crawler tool and crawlered 600G+ data. Second, I have tried statistic model, extracting a bunch of features from
        articles and training a linear regression model to predict the quality. This method worked well but was very basic
        and had no difference with previous works. Since the history of wikipedia article is
        quite like the sentences in language model and inspired by <a href="http://www.opensym.org/os2016/proceedings-files/p600-agrawal.pdf">Agrawal's work</a> last year, I'm trying neural
        model to predict article quality.
    </p>
</div>
</body>
</html>