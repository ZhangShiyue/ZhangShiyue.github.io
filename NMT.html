<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>NMT</title>
     <link rel="stylesheet" href="css/templatemo-style.css">
</head>
<body>
<h2 class="tm-text-title">NMT</h2>

<img src="img/m-nmt.png" style="width: 800px"> <br>
<div class="tm-bg-white-translucent text-xs-left tm-textbox tm-2-col-textbox">
    <p class="tm-text">
        Figure above shows our M-NMT structure. This work has been submitted to ACL17, but it is a hurry version.
        We are still working on and improving it. In the figure, left part is a memory component that memorizes the
        possible target words of source words in current sentence. And we compute the attention on memory, similar to
        original attention model. Then, use memory attention to influence the decoding process. The effects of memory
        come from two aspects: one is memory narrows down the target word candidates to select from, this also
        was proved effective in <a href="http://www.phontron.com/paper/arthur16emnlp.pdf">another work</a>; the other is we can take advantage of SMT results (word mappings)
        in NMT. NMT is good at learning logic and meanings and SMT is good at learning word mappings, our work
        gives a way to take advantage of both of them.
    </p>

    <p class="tm-text">
        I'm coauthored with and supervised by Dr. Yang Feng and Prof. Dong Wang in this work. Reproduced baseline model
        on tensorflow, and working on improving memory attention model. I have implemented beam search on original seq2seq
        model of tensorflow, and found a initialization bug of the code, see the <a href="https://github.com/ZhangShiyue/translate">repos</a>.
        When I was reproducing the <a href="https://arxiv.org/pdf/1409.0473v1.pdf">Attention-based NMT</a> baseline model, the original theano code is lack of a testing
        part, I added it, see the 'add_test' branch in <a href="https://github.com/ZhangShiyue/blocks-examples">repos</a>.
    </p>
</div>
</body>
</html>