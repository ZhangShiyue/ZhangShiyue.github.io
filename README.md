# Hello World!

<img src="./img/shiyueatnaacl.jpeg" alt="shiyueatnaacl" width="400"/>

I am Shiyue Zhang (张诗悦). I have been a Research Scientist/Engineer at [Bloomberg AI](https://www.bloomberg.com/company/values/tech-at-bloomberg/artificial-intelligence-ai/) since August 2023. 

Previously, I was a Ph.D. student at UNC Chapel Hill, advised by Prof. [Mohit Bansal](https://www.cs.unc.edu/~mbansal/) and partially sponsored by [Bloomberg Data Science PhD Fellowship](https://www.bloomberg.com/company/stories/announcing-bloomberg-data-science-ph-d-fellowship-winners-2021-2022/).

## Research
I am interested in any fun topics related to language modeling. 

See the complete list of my publications at [Google Scholar](https://scholar.google.com/citations?user=co9KUGQAAAAJ&hl=en) and my <a href="files/Shiyue_Zhang_CV.pdf">CV</a>.

## Recent Publications
[Evaluating the Retrieval Robustness of Large Language Models](https://arxiv.org/pdf/2505.21870), 2025 <br> Shuyang Cao, Karthik Radhakrishnan, David Rosenberg, Steven Lu, Pengxiang Cheng, Lu Wang, **Shiyue
Zhang**

[Improving Instruct Models for Free: A Study on Partial Adaptation](https://arxiv.org/abs/2504.11626), EMNLP 2025 <br> Ozan İrsoy*, Pengxiang Cheng*, Jennifer L. Chen*, Daniel Preoţiuc-Pietro*, **Shiyue Zhang**\*, Duccio Pappadopulo* (*author ordering chosen at random)

[Localizing Factual Inconsistencies in Attributable Text Generation](https://arxiv.org/abs/2410.07473), TACL 2025 <br> Arie Cattan, Paul Roit, **Shiyue Zhang**, David Wan, Roee Aharoni, Idan Szpektor, Mohit Bansal, Ido Dagan

[QAPyramid: Fine-grained Evaluation of Content Selection for Text Summarization](https://arxiv.org/abs/2412.07096), COLM 2025 <br> **Shiyue Zhang**\*, David Wan*, Arie Cattan, Ayal Klein, Ido Dagan, Mohit Bansal (*equal contribution)

[RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models](https://arxiv.org/abs/2504.18041), NAACL 2025 <br> Bang An, **Shiyue Zhang**, Mark Dredze

[MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies](https://arxiv.org/abs/2305.16958), ACL 2023 <br> **Shiyue Zhang**, Shijie Wu, Ozan İrsoy, Steven Lu, Mohit Bansal, Mark Dredze, David Rosenberg

[Extractive is not Faithful: An Investigation of Broad Unfaithfulness Problems in Extractive Summarization](https://arxiv.org/abs/2209.03549), ACL 2023 <br> **Shiyue Zhang**\*, David Wan*, Mohit Bansal (*equal contribution)

[Evaluating the Factual Consistency of Large Language Models Through News Summarization](https://arxiv.org/abs/2211.08412), ACL 2023 Findings <br> Derek Tam, Anisha Mascarenhas, **Shiyue Zhang**, Sarah Kwan, Mohit Bansal, Colin Raffel

[HistAlign: Improving Context Dependency in Language Generation by Aligning with History](https://arxiv.org/abs/2305.04782), EMNLP 2023 <br> David Wan, **Shiyue Zhang**, Mohit Bansal


## Education
Aug. 2018 - Aug. 2023: Ph.D. in Computer Science, University of North Carolina (UNC) at Chapel Hill, USA, 

Sep. 2015 - March 2018: Masters in Information and Communication Engineering, Beijing University of Posts and Telecommunications (BUPT), China

Sep. 2011 - July 2015:  Bachelors in Communication Engineering, Beijing University of Posts and Telecommunications (BUPT), China

## Work Experience
Aug 2023 – Present: Research Engineer, Bloomberg

May 2022 – Aug. 2022: Software Engineer Intern, Bloomberg

May 2021 – Aug. 2021: Research Intern, Facebook AI

June 2020 – Aug. 2020: Research Intern, Microsoft Research 

Sep. 2016 – June 2018: Research Intern, Center for Speech and Language Technologies at Tsinghua University

## Contact

I am based in NYC. Contact me via byryuer@gmail.com.

## Miscellaneous

I like to write non-technical blogs occasionally and will post them on [matters.town/@yuerrrr](https://matters.town/@yuerrrr). 